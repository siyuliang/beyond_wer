{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57b8750-640d-48e1-a065-729341c7016c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 01:05:51,812 [INFO] Starting average token entropy analysis (K_H=50)\n",
      "2025-05-20 01:05:51,812 [INFO] Data directory: results_beam_600s\n",
      "2025-05-20 01:05:51,813 [INFO] Training hours file: whisper_training_hours.csv\n",
      "2025-05-20 01:05:51,813 [INFO] Output directory: analysis_results_beam\n",
      "2025-05-20 01:05:51,821 [INFO] Loaded training hours for 90 languages\n",
      "2025-05-20 01:05:51,822 [INFO] Found 28 potential language CSV files.\n",
      "2025-05-20 01:06:12,461 [INFO] Saved metrics to analysis_results_beam/language_entropy_kH50_metrics.csv\n",
      "2025-05-20 01:06:12,463 [INFO] Creating entropy plot...\n",
      "2025-05-20 01:06:13,448 [INFO] Saved entropy plot to analysis_results_beam (PNG, PDF, SVG)\n",
      "2025-05-20 01:06:13,450 [INFO] Entropy analysis complete!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Whisper Analysis: Average Token Entropy vs. Training Hours\n",
    "===========================================================\n",
    "\n",
    "Generates a plot of average token entropy (uncertainty)\n",
    "against Whisper training hours for various languages.\n",
    "Entropy is calculated over the top K_H probabilities.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Configuration\n",
    "# ────────────────────────────────────────────────────────────\n",
    "DATA_DIR = Path(\"results_beam_600s\")\n",
    "TRAINING_HOURS_CSV = Path(\"whisper_training_hours.csv\")\n",
    "OUTPUT_DIR = Path(\"analysis_results_beam\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "K_H = 50  # Number of top candidates for entropy calculation\n",
    "\n",
    "# Language resource groups\n",
    "HIGH_RESOURCE = {\"de\", \"es\", \"fr\", \"pt\", \"tr\"}\n",
    "MEDIUM_RESOURCE = {\"it\", \"nl\", \"sv\", \"ca\", \"fi\", \"id\", \"vi\", \"ro\",\n",
    "                   \"no\", \"cs\", \"hu\"}\n",
    "LOW_RESOURCE = {\"cy\", \"lt\", \"lv\", \"az\", \"et\", \"eu\", \"sq\", \"sw\", \"mt\", \"uz\"}\n",
    "\n",
    "EXCLUDED_LANGUAGES = {\"uz\", \"mt\", \"sw\", \"sq\", \"yo\", \"da\", \"vi\", \"cs\"}\n",
    "\n",
    "# Visual styling\n",
    "COLOURS = {\"High\": \"steelblue\", \"Medium\": \"seagreen\",\n",
    "           \"Low\": \"crimson\", \"Other\": \"grey\"}\n",
    "MARKERS = {\"High\": \"o\", \"Medium\": \"^\", \"Low\": \"s\", \"Other\": \"x\"}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 100,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"font.size\": 8,\n",
    "    \"axes.titlesize\": 10,\n",
    "    \"axes.labelsize\": 8,\n",
    "    \"xtick.labelsize\": 7,\n",
    "    \"ytick.labelsize\": 7,\n",
    "    \"legend.fontsize\": 7,\n",
    "    \"figure.titlesize\": 12\n",
    "})\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Utility functions\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def get_resource_group(code):\n",
    "    \"\"\"Determine resource group based on language code.\"\"\"\n",
    "    if code in HIGH_RESOURCE:\n",
    "        return \"High\"\n",
    "    if code in MEDIUM_RESOURCE:\n",
    "        return \"Medium\"\n",
    "    if code in LOW_RESOURCE:\n",
    "        return \"Low\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "def load_training_hours():\n",
    "    \"\"\"Load training hours from CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(TRAINING_HOURS_CSV)\n",
    "        df = df[df[\"Whisper_Training_Hours\"].notna()]\n",
    "        df = df[df[\"Whisper_Training_Hours\"] != \"Unknown\"]\n",
    "        \n",
    "        hours_dict = {}\n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                code = row[\"Whisper_Code\"]\n",
    "                hours = float(row[\"Whisper_Training_Hours\"])\n",
    "                hours_dict[code] = hours\n",
    "            except (ValueError, KeyError):\n",
    "                logging.warning(f\"Skipping row due to missing code or invalid hours: {row}\")\n",
    "                continue\n",
    "                \n",
    "        logging.info(f\"Loaded training hours for {len(hours_dict)} languages\")\n",
    "        return hours_dict\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Training hours file not found: {TRAINING_HOURS_CSV}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load training hours: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def safe_read_csv(path):\n",
    "    \"\"\"Safely read a CSV file, handling potential errors.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(f\"File not found: {path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to read {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Core analysis functions\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def calculate_avg_entropy(df, k_h_val):\n",
    "    \"\"\"Calculate the average token entropy H_s based on top k_h_val probabilities.\"\"\"\n",
    "    if 'step' in df.columns:\n",
    "        pred_rows = df[df['step'] >= 0].copy()\n",
    "    else:\n",
    "        logging.warning(\"No 'step' column found. Using all rows for entropy calculation.\")\n",
    "        pred_rows = df.copy()\n",
    "    \n",
    "    if pred_rows.empty:\n",
    "        logging.debug(\"No prediction rows found for entropy calculation\")\n",
    "        return np.nan\n",
    "        \n",
    "    prob_cols = [f'top{k}_prob' for k in range(1, k_h_val + 1)]\n",
    "    \n",
    "    if 'top1_prob' not in pred_rows.columns:\n",
    "        logging.warning(f\"Missing 'top1_prob' column. Cannot calculate entropy.\")\n",
    "        return np.nan\n",
    "\n",
    "    entropies_for_file = []\n",
    "    for _, row in pred_rows.iterrows():\n",
    "        probs_kH = []\n",
    "        for col_name in prob_cols:\n",
    "            prob_val = row.get(col_name)\n",
    "            if pd.notna(prob_val):\n",
    "                probs_kH.append(prob_val)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not probs_kH:\n",
    "            continue\n",
    "\n",
    "        probs_kH = [p for p in probs_kH if p > 0]\n",
    "        if not probs_kH:\n",
    "            entropies_for_file.append(0.0)\n",
    "            continue\n",
    "\n",
    "        sum_probs_kH = sum(probs_kH)\n",
    "        \n",
    "        if sum_probs_kH == 0:\n",
    "            entropies_for_file.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        normalized_probs = [p / sum_probs_kH for p in probs_kH]\n",
    "        \n",
    "        entropy_s = 0.0\n",
    "        for p_prime in normalized_probs:\n",
    "            if p_prime > 0:\n",
    "                entropy_s -= p_prime * np.log2(p_prime)\n",
    "        \n",
    "        entropies_for_file.append(entropy_s)\n",
    "        \n",
    "    if not entropies_for_file:\n",
    "        return np.nan\n",
    "        \n",
    "    return np.mean(entropies_for_file)\n",
    "\n",
    "\n",
    "def process_language_file(file_path, training_hours, k_h_val):\n",
    "    \"\"\"Process a single language file and extract metrics including entropy.\"\"\"\n",
    "    try:\n",
    "        filename = os.path.basename(file_path)\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) > 1 and parts[0].isdigit():\n",
    "            lang_name = parts[1].capitalize()\n",
    "        else:\n",
    "            lang_name = parts[0].split('.')[0].capitalize()\n",
    "            \n",
    "        logging.debug(f\"Processing {lang_name} from {filename}...\")\n",
    "        \n",
    "        df = safe_read_csv(file_path)\n",
    "        \n",
    "        if df.empty:\n",
    "            logging.warning(f\"Empty or invalid data for {lang_name} from {filename}\")\n",
    "            return None\n",
    "            \n",
    "        code = None\n",
    "        if 'whisper_lang' in df.columns:\n",
    "            codes = df['whisper_lang'].dropna().unique()\n",
    "            if len(codes) >= 1:\n",
    "                code = codes[0]\n",
    "                if len(codes) > 1:\n",
    "                    logging.warning(f\"Multiple whisper_lang codes in {filename}: {codes}. Using first: {code}\")\n",
    "\n",
    "        avg_entropy = calculate_avg_entropy(df, k_h_val)\n",
    "        \n",
    "        num_utterances = 0\n",
    "        if not df.empty:\n",
    "            if \"step\" in df.columns:\n",
    "                num_utterances = int((df[\"step\"] == -1).sum())\n",
    "            else:\n",
    "                num_utterances = len(df)\n",
    "\n",
    "        return {\n",
    "            \"language\": lang_name,\n",
    "            \"code\": code,\n",
    "            \"training_hours\": training_hours.get(code, np.nan) if code else np.nan,\n",
    "            \"num_utterances\": num_utterances,\n",
    "            \"avg_entropy\": avg_entropy,\n",
    "            \"resource_group\": get_resource_group(code)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def collect_all_results(k_h_val):\n",
    "    \"\"\"Collect results (including entropy) from all language files.\"\"\"\n",
    "    training_hours = load_training_hours()\n",
    "    \n",
    "    csv_files = sorted(DATA_DIR.glob(\"*_subtoken_beam.csv\"))\n",
    "\n",
    "    if not csv_files:\n",
    "        logging.error(f\"No relevant CSV files found in {DATA_DIR} (e.g., *_subtoken_beam.csv).\")\n",
    "        try:\n",
    "            all_files = list(DATA_DIR.glob(\"*.csv\"))\n",
    "            if all_files:\n",
    "                logging.info(f\"Found these CSV files: {', '.join(f.name for f in all_files[:10])}...\")\n",
    "            else:\n",
    "                logging.info(f\"Directory {DATA_DIR} contains no CSV files.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error listing directory contents: {e}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    logging.info(f\"Found {len(csv_files)} potential language CSV files.\")\n",
    "    \n",
    "    results = []\n",
    "    for file_path in csv_files:\n",
    "        result = process_language_file(file_path, training_hours, k_h_val)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    \n",
    "    if not results:\n",
    "        logging.error(\"No valid results collected from CSV files.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    try:\n",
    "        output_file = OUTPUT_DIR / f\"language_entropy_kH{k_h_val}_metrics.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Saved metrics to {output_file}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save metrics CSV: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Plotting functions\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def plot_entropy_vs_hours(results_df, k_h_val):\n",
    "    \"\"\"Plot average token entropy vs training hours.\"\"\"\n",
    "    df = results_df[~results_df[\"code\"].isin(EXCLUDED_LANGUAGES)].copy()\n",
    "    df = df.dropna(subset=[\"avg_entropy\", \"training_hours\"])\n",
    "    \n",
    "    if df.empty:\n",
    "        logging.warning(\"No valid data for average entropy plot after filtering and dropping NaNs.\")\n",
    "        fig, ax = plt.subplots(figsize=(4.5, 3.5))\n",
    "        ax.text(0.5, 0.5, \"No data to plot for entropy vs. hours.\",\n",
    "                horizontalalignment='center', verticalalignment='center',\n",
    "                transform=ax.transAxes, fontsize=10)\n",
    "        ax.set_xlabel(\"Whisper Training Hours (log scale)\")\n",
    "        ax.set_ylabel(f\"Average Token Entropy (bits, $K_H={k_h_val}$)\")\n",
    "        try:\n",
    "            output_plot_file_png = OUTPUT_DIR / f\"avg_entropy_kH{k_h_val}_vs_hours_nodata.png\"\n",
    "            fig.savefig(output_plot_file_png)\n",
    "            logging.info(f\"Saved empty plot placeholder to {output_plot_file_png}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save empty entropy plot: {e}\")\n",
    "        plt.close(fig)\n",
    "        return\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(4.5, 3.5))\n",
    "    \n",
    "    for group_name in [\"High\", \"Medium\", \"Low\", \"Other\"]:\n",
    "        group_df = df[df[\"resource_group\"] == group_name]\n",
    "        if group_df.empty:\n",
    "            continue\n",
    "        \n",
    "        marker = MARKERS.get(group_name, \"x\")\n",
    "        color = COLOURS.get(group_name, \"grey\")\n",
    "\n",
    "        ax.scatter(\n",
    "            group_df[\"training_hours\"],\n",
    "            group_df[\"avg_entropy\"],\n",
    "            s=40,\n",
    "            alpha=0.7,\n",
    "            marker=marker,\n",
    "            color=color,\n",
    "            label=f\"{group_name}-resource\"\n",
    "        )\n",
    "        \n",
    "        for _, row in group_df.iterrows():\n",
    "            ax.annotate(\n",
    "                row[\"language\"],\n",
    "                (row[\"training_hours\"], row[\"avg_entropy\"]),\n",
    "                xytext=(3, 1),\n",
    "                textcoords=\"offset points\",\n",
    "                fontsize=6,\n",
    "            )\n",
    "    \n",
    "    # Calculate trend line\n",
    "    if len(df) >= 2:\n",
    "        valid_mask = (df[\"training_hours\"].astype(float) > 0) & \\\n",
    "                     np.isfinite(np.log10(df[\"training_hours\"].astype(float))) & \\\n",
    "                     np.isfinite(df[\"avg_entropy\"])\n",
    "        \n",
    "        if sum(valid_mask) >= 2:\n",
    "            x_log = np.log10(df.loc[valid_mask, \"training_hours\"].astype(float))\n",
    "            y_vals = df.loc[valid_mask, \"avg_entropy\"]\n",
    "            \n",
    "            try:\n",
    "                z = np.polyfit(x_log, y_vals, 1)\n",
    "                p = np.poly1d(z)\n",
    "                r_val = np.corrcoef(x_log, y_vals)[0, 1]\n",
    "                \n",
    "                x_min_valid = df.loc[valid_mask, \"training_hours\"].min()\n",
    "                x_max_valid = df.loc[valid_mask, \"training_hours\"].max()\n",
    "\n",
    "                if x_min_valid > 0 and x_max_valid > 0 and x_min_valid < x_max_valid:\n",
    "                    x_line = np.logspace(np.log10(x_min_valid), np.log10(x_max_valid), 100)\n",
    "                    y_line = p(np.log10(x_line))\n",
    "                    ax.plot(x_line, y_line, 'k--', linewidth=1, alpha=0.7, label=f\"Trend (r={r_val:.2f})\")\n",
    "                else:\n",
    "                    logging.warning(\"Cannot plot trend line due to insufficient range.\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error fitting trend line: {e}\")\n",
    "        else:\n",
    "            logging.warning(\"Not enough valid data points to calculate trend line for entropy plot.\")\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Whisper Training Hours (log scale)\")\n",
    "    ax.set_ylabel(f\"Average Token Entropy\")\n",
    "    \n",
    "    # Dynamic Y-axis limits based on data range\n",
    "    valid_entropy_data = df[\"avg_entropy\"].dropna()\n",
    "    \n",
    "    if not valid_entropy_data.empty:\n",
    "        data_min_y = valid_entropy_data.min()\n",
    "        data_max_y = valid_entropy_data.max()\n",
    "\n",
    "        if data_min_y == data_max_y:\n",
    "            padding = 0.1 * abs(data_min_y) if data_min_y != 0 else 0.1\n",
    "            y_lower_bound = max(0, data_min_y - padding)\n",
    "            y_upper_bound = data_max_y + padding\n",
    "        else:\n",
    "            padding = (data_max_y - data_min_y) * 0.05\n",
    "            y_lower_bound = max(0, data_min_y - padding)\n",
    "            y_upper_bound = data_max_y + padding\n",
    "        \n",
    "        if y_upper_bound <= y_lower_bound:\n",
    "            y_upper_bound = y_lower_bound + 0.1\n",
    "\n",
    "        ax.set_ylim(y_lower_bound, y_upper_bound)\n",
    "    else:\n",
    "        y_min_default = 0.0\n",
    "        y_max_default = np.log2(k_h_val) if k_h_val > 1 else 1.0\n",
    "        \n",
    "        if y_max_default <= y_min_default:\n",
    "            y_max_default = y_min_default + 1.0\n",
    "        ax.set_ylim(y_min_default, y_max_default)\n",
    "    \n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    ax.legend()\n",
    "    \n",
    "    fig.tight_layout(pad=0.5)\n",
    "    \n",
    "    try:\n",
    "        fig.savefig(OUTPUT_DIR / f\"avg_entropy_kH{k_h_val}_vs_hours.png\")\n",
    "        fig.savefig(OUTPUT_DIR / f\"avg_entropy_kH{k_h_val}_vs_hours.pdf\")\n",
    "        fig.savefig(OUTPUT_DIR / f\"avg_entropy_kH{k_h_val}_vs_hours.svg\", format=\"svg\")\n",
    "        logging.info(f\"Saved entropy plot to {OUTPUT_DIR} (PNG, PDF, SVG)\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save entropy plot: {e}\")\n",
    "    \n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Main execution\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    logging.info(f\"Starting average token entropy analysis (K_H={K_H})\")\n",
    "    logging.info(f\"Data directory: {DATA_DIR}\")\n",
    "    logging.info(f\"Training hours file: {TRAINING_HOURS_CSV}\")\n",
    "    logging.info(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    \n",
    "    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    results_df = collect_all_results(K_H)\n",
    "    \n",
    "    if results_df.empty:\n",
    "        logging.error(\"No usable results found from data collection. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    if results_df[\"avg_entropy\"].notna().sum() == 0:\n",
    "        logging.warning(\"No valid average entropy data was found.\")\n",
    "        plot_entropy_vs_hours(results_df, K_H)\n",
    "        return\n",
    "        \n",
    "    logging.info(\"Creating entropy plot...\")\n",
    "    plot_entropy_vs_hours(results_df, K_H)\n",
    "    \n",
    "    logging.info(\"Entropy analysis complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (whisper_env)",
   "language": "python",
   "name": "whisper_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
