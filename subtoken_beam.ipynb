{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a27fd7e-3bdf-4727-8d64-27650597c1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siyu/.conda/envs/whisper_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper (large-v2) on device: NVIDIA A100-PCIE-40GB\n",
      "Whisper model loaded.\n",
      "\n",
      "─── [003/28] GERMAN (de / de) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 917350it [00:22, 41317.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 98 clips, 601.7s total for german\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding german: 100%|██████████| 98/98 [03:10<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/003_german_subtoken_beam.csv\n",
      "\n",
      "─── [004/28] SPANISH (es / es) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 415157it [00:09, 42742.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 101 clips, 602.7s total for spanish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding spanish:  40%|███▉      | 40/101 [01:14<01:51,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Segments not found for clip 40 (spanish). Re-tokenizing full_text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding spanish: 100%|██████████| 101/101 [03:39<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/004_spanish_subtoken_beam.csv\n",
      "\n",
      "─── [007/28] FRENCH (fr / fr) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 726005it [00:22, 32094.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 105 clips, 602.0s total for french\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding french:  14%|█▍        | 15/105 [00:28<02:26,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Segments not found for clip 15 (french). Re-tokenizing full_text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding french:  73%|███████▎  | 77/105 [02:17<00:46,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Segments not found for clip 77 (french). Re-tokenizing full_text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding french: 100%|██████████| 105/105 [03:05<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/007_french_subtoken_beam.csv\n",
      "\n",
      "─── [009/28] PORTUGUESE (pt / pt) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 152647it [00:03, 46667.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 119 clips, 603.3s total for portuguese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding portuguese: 100%|██████████| 119/119 [03:04<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/009_portuguese_subtoken_beam.csv\n",
      "\n",
      "─── [010/28] TURKISH (tr / tr) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 114056it [00:03, 35459.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 129 clips, 604.2s total for turkish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding turkish: 100%|██████████| 129/129 [03:24<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/010_turkish_subtoken_beam.csv\n",
      "\n",
      "─── [011/28] ITALIAN (it / it) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 236777it [00:09, 25735.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 104 clips, 607.0s total for italian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding italian: 100%|██████████| 104/104 [03:15<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/011_italian_subtoken_beam.csv\n",
      "\n",
      "─── [012/28] DUTCH (nl / nl) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 90449it [00:03, 27364.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 117 clips, 601.7s total for dutch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding dutch: 100%|██████████| 117/117 [03:24<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/012_dutch_subtoken_beam.csv\n",
      "\n",
      "─── [013/28] SWEDISH (sv / sv-SE) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 40770it [00:01, 27393.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 112 clips, 603.5s total for swedish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding swedish: 100%|██████████| 112/112 [03:23<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/013_swedish_subtoken_beam.csv\n",
      "\n",
      "─── [014/28] CATALAN (ca / ca) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 1824534it [01:02, 29174.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 95 clips, 601.8s total for catalan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding catalan: 100%|██████████| 95/95 [03:05<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/014_catalan_subtoken_beam.csv\n",
      "\n",
      "─── [015/28] FINNISH (fi / fi) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 10447it [00:00, 24901.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 112 clips, 601.6s total for finnish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding finnish: 100%|██████████| 112/112 [03:48<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/015_finnish_subtoken_beam.csv\n",
      "\n",
      "─── [016/28] INDONESIAN (id / id) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 26108it [00:01, 25112.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 136 clips, 601.9s total for indonesian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding indonesian:  80%|████████  | 109/136 [02:38<00:37,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Segments not found for clip 109 (indonesian). Re-tokenizing full_text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding indonesian: 100%|██████████| 136/136 [03:19<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/016_indonesian_subtoken_beam.csv\n",
      "\n",
      "─── [017/28] VIETNAMESE (vi / vi) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 5135it [00:00, 9787.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 140 clips, 604.5s total for vietnamese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding vietnamese: 100%|██████████| 140/140 [03:45<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/017_vietnamese_subtoken_beam.csv\n",
      "\n",
      "─── [018/28] ROMANIAN (ro / ro) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 17737it [00:00, 21752.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 137 clips, 600.3s total for romanian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding romanian: 100%|██████████| 137/137 [03:54<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/018_romanian_subtoken_beam.csv\n",
      "\n",
      "─── [019/28] DANISH (da / da) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 10225it [00:00, 30319.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 211 clips, 600.5s total for danish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding danish:  88%|████████▊ | 186/211 [02:31<00:19,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Segments not found for clip 186 (danish). Re-tokenizing full_text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding danish: 100%|██████████| 211/211 [02:50<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/019_danish_subtoken_beam.csv\n",
      "\n",
      "─── [020/28] NORWEGIAN (no / nn-NO) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 1173it [00:00, 3931.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 124 clips, 605.7s total for norwegian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding norwegian: 100%|██████████| 124/124 [03:35<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/020_norwegian_subtoken_beam.csv\n",
      "\n",
      "─── [021/28] CZECH (cs / cs) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 61391it [00:01, 47032.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 171 clips, 600.2s total for czech\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding czech: 100%|██████████| 171/171 [04:06<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/021_czech_subtoken_beam.csv\n",
      "\n",
      "─── [022/28] HUNGARIAN (hu / hu) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 60358it [00:03, 20093.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 101 clips, 607.4s total for hungarian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding hungarian: 100%|██████████| 101/101 [04:09<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/022_hungarian_subtoken_beam.csv\n",
      "\n",
      "─── [023/28] YORUBA (yo / yo) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 3077it [00:00, 6571.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 87 clips, 600.1s total for yoruba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding yoruba: 100%|██████████| 87/87 [03:42<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/023_yoruba_subtoken_beam.csv\n",
      "\n",
      "─── [035/28] LITHUANIAN (lt / lt) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 16643it [00:00, 25221.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 111 clips, 601.8s total for lithuanian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding lithuanian: 100%|██████████| 111/111 [04:14<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/035_lithuanian_subtoken_beam.csv\n",
      "\n",
      "─── [039/28] WELSH (cy / cy) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 90369it [00:01, 47477.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 100 clips, 600.3s total for welsh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding welsh: 100%|██████████| 100/100 [03:53<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/039_welsh_subtoken_beam.csv\n",
      "\n",
      "─── [043/28] LATVIAN (lv / lv) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 171652it [00:03, 46328.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 105 clips, 603.3s total for latvian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding latvian: 100%|██████████| 105/105 [04:23<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/043_latvian_subtoken_beam.csv\n",
      "\n",
      "─── [046/28] AZERBAIJANI (az / az) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 130it [00:00, 1012.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 115 clips, 601.0s total for azerbaijani\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding azerbaijani: 100%|██████████| 115/115 [04:48<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/046_azerbaijani_subtoken_beam.csv\n",
      "\n",
      "─── [049/28] ESTONIAN (et / et) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 24381it [00:01, 18162.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 81 clips, 601.6s total for estonian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding estonian: 100%|██████████| 81/81 [05:02<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/049_estonian_subtoken_beam.csv\n",
      "\n",
      "─── [052/28] BASQUE (eu / eu) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 179132it [00:06, 26277.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 100 clips, 603.0s total for basque\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding basque: 100%|██████████| 100/100 [03:51<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/052_basque_subtoken_beam.csv\n",
      "\n",
      "─── [059/28] ALBANIAN (sq / sq) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 1524it [00:00, 4959.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 122 clips, 605.2s total for albanian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding albanian: 100%|██████████| 122/122 [04:23<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/059_albanian_subtoken_beam.csv\n",
      "\n",
      "─── [060/28] SWAHILI (sw / sw) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 267001it [00:10, 26219.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 104 clips, 605.4s total for swahili\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding swahili:  47%|████▋     | 49/104 [02:00<01:41,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Segments not found for clip 49 (swahili). Re-tokenizing full_text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding swahili:  86%|████████▌ | 89/104 [03:26<00:39,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Segments not found for clip 89 (swahili). Re-tokenizing full_text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding swahili: 100%|██████████| 104/104 [04:18<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/060_swahili_subtoken_beam.csv\n",
      "\n",
      "─── [079/28] UZBEK (uz / uz) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 86430it [00:01, 49121.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 109 clips, 603.7s total for uzbek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding uzbek: 100%|██████████| 109/109 [04:40<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/079_uzbek_subtoken_beam.csv\n",
      "\n",
      "─── [085/28] MALTESE (mt / mt) ───────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 6442it [00:00, 11225.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing 110 clips, 600.7s total for maltese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding maltese: 100%|██████████| 110/110 [05:53<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV written → results_beam_600s/085_maltese_subtoken_beam.csv\n",
      "✓ Temporary directory results_beam_600s/tmp_audio removed.\n",
      "\n",
      "✓ All 28/28 language tasks attempted.\n",
      "  Successfully processed: 28 languages.\n",
      "  Output files are in: results_beam_600s\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Whisper sub-token dump (top-50 per decoding step) for multiple languages (Beam Search with Rank)\n",
    "================================================================================================\n",
    "For each specified language:\n",
    "  • Streams ≈600 s of Common Voice 17.0 audio\n",
    "  • Forces the <|lang|> token so Whisper skips language-ID detection\n",
    "  • Transcribes using BEAM SEARCH (beam_size=5, temperature=0.2)\n",
    "  • Decodes step-by-step ALONG THE BEAM SEARCH PATH and records top-K candidate IDs + strings\n",
    "  • Adds a 'chosen_rank' column indicating the rank of the chosen token in that step's top-K predictions\n",
    "  • EVERY row carries `audio_path`, `ground_truth`, `full_transcription`,\n",
    "    and `whisper_lang`, so merging is unnecessary\n",
    "  • A sentinel row with step = -1 stores utterance-level metadata\n",
    "  • One CSV per language is written to results_beam_600s/\n",
    "\"\"\"\n",
    "\n",
    "# ───────────────────────── Imports ────────────────────────────\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "\n",
    "import torch\n",
    "import whisper\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Audio\n",
    "from whisper.tokenizer import get_tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Silence a harmless HF warning about trust_remote_code\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ──────────────── Language configuration ─────────────────────────\n",
    "LANGUAGES = {\n",
    "    # Format: \"language_name\": {\"num\": index, \"code\": whisper_code, \"cv\": cv_code}\n",
    "    # High-resource languages (more than ~10k hours)\n",
    "    \"german\": {\"num\": 3, \"code\": \"de\", \"cv\": \"de\"},\n",
    "    \"spanish\": {\"num\": 4, \"code\": \"es\", \"cv\": \"es\"},\n",
    "    \"french\": {\"num\": 7, \"code\": \"fr\", \"cv\": \"fr\"},\n",
    "    \"portuguese\": {\"num\": 9, \"code\": \"pt\", \"cv\": \"pt\"},\n",
    "    \"turkish\": {\"num\": 10, \"code\": \"tr\", \"cv\": \"tr\"},\n",
    "    \n",
    "    # Medium-resource languages (between 100 and ~10k hours)\n",
    "    \"italian\": {\"num\": 11, \"code\": \"it\", \"cv\": \"it\"},\n",
    "    \"dutch\": {\"num\": 12, \"code\": \"nl\", \"cv\": \"nl\"},\n",
    "    \"swedish\": {\"num\": 13, \"code\": \"sv\", \"cv\": \"sv-SE\"},\n",
    "    \"catalan\": {\"num\": 14, \"code\": \"ca\", \"cv\": \"ca\"},\n",
    "    \"finnish\": {\"num\": 15, \"code\": \"fi\", \"cv\": \"fi\"},\n",
    "    \"indonesian\": {\"num\": 16, \"code\": \"id\", \"cv\": \"id\"},\n",
    "    \"vietnamese\": {\"num\": 17, \"code\": \"vi\", \"cv\": \"vi\"},\n",
    "    \"romanian\": {\"num\": 18, \"code\": \"ro\", \"cv\": \"ro\"},\n",
    "    \"danish\": {\"num\": 19, \"code\": \"da\", \"cv\": \"da\"},\n",
    "    \"norwegian\": {\"num\": 20, \"code\": \"no\", \"cv\": \"nn-NO\"},\n",
    "    \"czech\": {\"num\": 21, \"code\": \"cs\", \"cv\": \"cs\"},\n",
    "    \"hungarian\": {\"num\": 22, \"code\": \"hu\", \"cv\": \"hu\"},\n",
    "    \"yoruba\": {\"num\": 23, \"code\": \"yo\", \"cv\": \"yo\"},\n",
    "    \n",
    "    # Low-resource languages (less than 100 hours)\n",
    "    \"welsh\": {\"num\": 39, \"code\": \"cy\", \"cv\": \"cy\"},\n",
    "    \"lithuanian\": {\"num\": 35, \"code\": \"lt\", \"cv\": \"lt\"},\n",
    "    \"latvian\": {\"num\": 43, \"code\": \"lv\", \"cv\": \"lv\"},\n",
    "    \"azerbaijani\": {\"num\": 46, \"code\": \"az\", \"cv\": \"az\"},\n",
    "    \"estonian\": {\"num\": 49, \"code\": \"et\", \"cv\": \"et\"},\n",
    "    \"basque\": {\"num\": 52, \"code\": \"eu\", \"cv\": \"eu\"},\n",
    "    \"albanian\": {\"num\": 59, \"code\": \"sq\", \"cv\": \"sq\"},\n",
    "    \"swahili\": {\"num\": 60, \"code\": \"sw\", \"cv\": \"sw\"},\n",
    "    \"maltese\": {\"num\": 85, \"code\": \"mt\", \"cv\": \"mt\"},\n",
    "    \"uzbek\": {\"num\": 79, \"code\": \"uz\", \"cv\": \"uz\"},\n",
    "}\n",
    "\n",
    "# ───────────── Global configuration ──────────────────────────────\n",
    "TOP_K = 50\n",
    "MAX_DECODER_INPUT_TOKENS = 448  # Max tokens for decoder input sequence (context window)\n",
    "TARGET_SEC = 600  # 10 minutes of audio per language\n",
    "BEAM_SIZE = 5\n",
    "TEMPERATURE = 0.2\n",
    "\n",
    "OUT_DIR = Path(f\"results_beam_{TARGET_SEC}s\")\n",
    "TEMP_DIR = OUT_DIR / \"tmp_audio\"\n",
    "ERROR_LOG = OUT_DIR / \"errors.log\"\n",
    "\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "TEMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "if ERROR_LOG.exists():\n",
    "    ERROR_LOG.unlink()\n",
    "\n",
    "# ─────────────── Whisper model (shared) ──────────────────────────\n",
    "print(f\"Loading Whisper (large-v2) on device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = whisper.load_model(\"large-v2\", device=device)\n",
    "print(\"Whisper model loaded.\")\n",
    "\n",
    "\n",
    "# ─────────── Helper: stream ~TARGET_SEC of audio ─────────────────\n",
    "def load_some_cv_audio(cv_code: str, seconds: int):\n",
    "    \"\"\"\n",
    "    Load approximately `seconds` worth of audio from Common Voice 17.0.\n",
    "    Tries validated, train, and test splits in order.\n",
    "    \"\"\"\n",
    "    splits = [\"validated\", \"train\", \"test\"]\n",
    "    for split in splits:\n",
    "        try:\n",
    "            ds = load_dataset(\n",
    "                \"mozilla-foundation/common_voice_17_0\",\n",
    "                cv_code,\n",
    "                split=split,\n",
    "                streaming=True,\n",
    "                trust_remote_code=True,\n",
    "            ).cast_column(\"audio\", Audio(16000))\n",
    "            \n",
    "            batch, dur = [], 0.0\n",
    "            for ex in ds:\n",
    "                raw = ex[\"path\"]\n",
    "                ex[\"cv_path\"] = f\"{cv_code}_{split}/{Path(raw).name}\"\n",
    "                arr = ex[\"audio\"][\"array\"]\n",
    "                dur += len(arr) / 16000\n",
    "                batch.append(ex)\n",
    "                if dur >= seconds:\n",
    "                    return batch\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Error loading {cv_code} {split}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"No audio found for {cv_code} after trying all splits.\")\n",
    "    return []\n",
    "\n",
    "\n",
    "def log_error_to_file(language, error_msg):\n",
    "    \"\"\"Log error details to the error log file.\"\"\"\n",
    "    with open(ERROR_LOG, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"Language: {language}\\nError: {error_msg}\\n\\n\")\n",
    "\n",
    "\n",
    "# ───────────────────── Main loop ─────────────────────────────────\n",
    "total_langs_to_process = len(LANGUAGES)\n",
    "processed_langs_count = 0\n",
    "successful_langs_count = 0\n",
    "failed_langs_count = 0\n",
    "\n",
    "for lang_name, info in sorted(LANGUAGES.items(), key=lambda x: x[1][\"num\"]):\n",
    "    processed_langs_count += 1\n",
    "    lang_num = info[\"num\"]\n",
    "    lang_code = info[\"code\"]\n",
    "    cv_code = info[\"cv\"]\n",
    "\n",
    "    print(f\"\\n─── [{lang_num:03d}/{total_langs_to_process}] {lang_name.upper()} ({lang_code} / {cv_code}) ───────────\")\n",
    "\n",
    "    try:\n",
    "        samples = load_some_cv_audio(cv_code, TARGET_SEC)\n",
    "        if not samples:\n",
    "            print(f\"⚠️  No audio found for {lang_name} – skipping.\")\n",
    "            log_error_to_file(lang_name, \"No audio found after trying all splits.\")\n",
    "            failed_langs_count += 1\n",
    "            continue\n",
    "\n",
    "        total_dur = sum(len(s[\"audio\"][\"array\"]) / 16000 for s in samples)\n",
    "        print(f\"✓ Processing {len(samples)} clips, {total_dur:.1f}s total for {lang_name}\")\n",
    "\n",
    "        tok = get_tokenizer(multilingual=True, language=lang_code, task=\"transcribe\")\n",
    "        lang_token_id = tok.encode(f\"<|{lang_code}|>\", allowed_special=\"all\")[0]\n",
    "        transcribe_token_id = tok.encode(\"<|transcribe|>\", allowed_special=\"all\")[0]\n",
    "        sot_token_id = tok.sot\n",
    "\n",
    "        rows = []\n",
    "        for clip_id, ex in enumerate(tqdm(samples, desc=f\"Decoding {lang_name}\")):\n",
    "            cv_path = ex[\"cv_path\"]\n",
    "            ground_truth = ex.get(\"sentence\", \"\").strip()\n",
    "\n",
    "            wav_path = TEMP_DIR / f\"{lang_name}_{cv_code}_{clip_id}.wav\"\n",
    "            sf.write(wav_path, ex[\"audio\"][\"array\"], 16000, subtype='PCM_16')\n",
    "\n",
    "            # Perform transcription using beam search\n",
    "            res = model.transcribe(\n",
    "                str(wav_path),\n",
    "                language=lang_code,\n",
    "                task=\"transcribe\",\n",
    "                beam_size=BEAM_SIZE,\n",
    "                temperature=TEMPERATURE,\n",
    "                verbose=None\n",
    "            )\n",
    "            \n",
    "            full_text = res[\"text\"].strip()\n",
    "            lang_guess = res[\"language\"]\n",
    "\n",
    "            # Extract beam search output tokens\n",
    "            beam_search_output_tokens = []\n",
    "            if \"segments\" in res and res[\"segments\"]:\n",
    "                for segment in res[\"segments\"]:\n",
    "                    beam_search_output_tokens.extend(segment['tokens'])\n",
    "            else:\n",
    "                print(f\"Warning: Segments not found for clip {clip_id} ({lang_name}). Re-tokenizing full_text.\")\n",
    "                beam_search_output_tokens = tok.encode(full_text)\n",
    "            \n",
    "            # Ensure EOT is included if missing\n",
    "            if not beam_search_output_tokens or beam_search_output_tokens[-1] != tok.eot:\n",
    "                if len(beam_search_output_tokens) < MAX_DECODER_INPUT_TOKENS - 1:\n",
    "                    beam_search_output_tokens.append(tok.eot)\n",
    "                else:\n",
    "                    beam_search_output_tokens[-1] = tok.eot\n",
    "\n",
    "            # Add sentinel row with utterance-level metadata\n",
    "            rows.append({\n",
    "                \"clip\": clip_id,\n",
    "                \"step\": -1,\n",
    "                \"audio_path\": cv_path,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"full_transcription\": full_text,\n",
    "                \"whisper_lang\": lang_guess,\n",
    "                \"chosen_rank\": None\n",
    "            })\n",
    "\n",
    "            # Prepare encoder output\n",
    "            audio_features = whisper.load_audio(str(wav_path))\n",
    "            mel_segment = whisper.log_mel_spectrogram(whisper.pad_or_trim(audio_features)).to(device)\n",
    "            encoder_output = model.encoder(mel_segment.unsqueeze(0))\n",
    "\n",
    "            current_decoder_input_token_ids = [sot_token_id, lang_token_id, transcribe_token_id]\n",
    "\n",
    "            # Step through each token in the beam search output\n",
    "            for step_idx, actual_next_token_id in enumerate(beam_search_output_tokens):\n",
    "                if len(current_decoder_input_token_ids) >= MAX_DECODER_INPUT_TOKENS:\n",
    "                    print(f\"Warning: Decoder input sequence for clip {clip_id}, step {step_idx} \"\n",
    "                          f\"reached {len(current_decoder_input_token_ids)} tokens. Breaking step-by-step.\")\n",
    "                    break\n",
    "                \n",
    "                decoder_input_tensor = torch.tensor([current_decoder_input_token_ids], device=device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits_for_next_token = model.decoder(decoder_input_tensor, encoder_output)[0, -1]\n",
    "                \n",
    "                probabilities_for_next_token = torch.softmax(logits_for_next_token, dim=-1)\n",
    "                top_k_probs, top_k_ids = torch.topk(probabilities_for_next_token, TOP_K)\n",
    "\n",
    "                # Get probability of the chosen token\n",
    "                chosen_token_prob = 0.0\n",
    "                if actual_next_token_id < probabilities_for_next_token.shape[0]:\n",
    "                    chosen_token_prob = probabilities_for_next_token[actual_next_token_id].item()\n",
    "                \n",
    "                # Calculate rank of the chosen token\n",
    "                chosen_rank = TOP_K + 1  # Default if not in top_k\n",
    "                try:\n",
    "                    chosen_rank = top_k_ids.tolist().index(actual_next_token_id) + 1\n",
    "                except ValueError:\n",
    "                    pass  # Remains TOP_K + 1\n",
    "\n",
    "                row_data = {\n",
    "                    \"clip\": clip_id,\n",
    "                    \"step\": step_idx,\n",
    "                    \"audio_path\": cv_path,\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                    \"full_transcription\": full_text,\n",
    "                    \"whisper_lang\": lang_guess,\n",
    "                    \"chosen_id\": actual_next_token_id,\n",
    "                    \"chosen_prob\": chosen_token_prob,\n",
    "                    \"chosen_rank\": chosen_rank,\n",
    "                }\n",
    "                \n",
    "                # Add top-K predictions\n",
    "                for k_val in range(TOP_K):\n",
    "                    if k_val < len(top_k_ids):\n",
    "                        token_id_k = top_k_ids[k_val].item()\n",
    "                        row_data[f\"top{k_val+1}_id\"] = token_id_k\n",
    "                        row_data[f\"top{k_val+1}_txt\"] = tok.decode([token_id_k])\n",
    "                        row_data[f\"top{k_val+1}_prob\"] = float(top_k_probs[k_val].item())\n",
    "                    else:\n",
    "                        row_data[f\"top{k_val+1}_id\"] = None\n",
    "                        row_data[f\"top{k_val+1}_txt\"] = \"\"\n",
    "                        row_data[f\"top{k_val+1}_prob\"] = 0.0\n",
    "                \n",
    "                rows.append(row_data)\n",
    "\n",
    "                current_decoder_input_token_ids.append(actual_next_token_id)\n",
    "                if actual_next_token_id == tok.eot:\n",
    "                    break\n",
    "            \n",
    "            if wav_path.exists():\n",
    "                wav_path.unlink()\n",
    "\n",
    "        # Write results to CSV\n",
    "        csv_path = OUT_DIR / f\"{lang_num:03d}_{lang_name}_subtoken_beam.csv\"\n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        # Reorder columns for better readability\n",
    "        cols = list(df.columns)\n",
    "        if \"chosen_rank\" in cols:\n",
    "            rank_idx = cols.index(\"chosen_rank\")\n",
    "            prob_idx = cols.index(\"chosen_prob\")\n",
    "            if rank_idx > prob_idx + 1:\n",
    "                cols.insert(prob_idx + 1, cols.pop(rank_idx))\n",
    "                df = df[cols]\n",
    "        \n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "        print(f\"✓ CSV written → {csv_path}\")\n",
    "        successful_langs_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        detailed_error_msg = f\"{type(e).__name__}: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        print(f\"❌ Error processing {lang_name}: {detailed_error_msg}\")\n",
    "        log_error_to_file(lang_name, detailed_error_msg)\n",
    "        failed_langs_count += 1\n",
    "        \n",
    "        # Clean up temp file if error occurred\n",
    "        if 'wav_path' in locals() and wav_path.exists():\n",
    "            wav_path.unlink()\n",
    "        continue\n",
    "\n",
    "# ───────────── Cleanup ───────────────────────────────────────────\n",
    "if TEMP_DIR.exists():\n",
    "    try:\n",
    "        shutil.rmtree(TEMP_DIR)\n",
    "        print(f\"✓ Temporary directory {TEMP_DIR} removed.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error removing temporary directory {TEMP_DIR}: {e.strerror}\")\n",
    "\n",
    "# ───────────── Summary ───────────────────────────────────────────\n",
    "print(f\"\\n✓ All {processed_langs_count}/{total_langs_to_process} language tasks attempted.\")\n",
    "print(f\"  Successfully processed: {successful_langs_count} languages.\")\n",
    "if failed_langs_count > 0:\n",
    "    print(f\"  Failed to process: {failed_langs_count} languages (see {ERROR_LOG} for details).\")\n",
    "print(f\"  Output files are in: {OUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (whisper_env)",
   "language": "python",
   "name": "whisper_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
