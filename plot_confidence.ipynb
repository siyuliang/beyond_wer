{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164112e-6051-417e-a5cb-6303e0230302",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:52:14,380 [INFO] Starting average confidence analysis\n",
      "2025-05-20 00:52:14,381 [INFO] Data directory: results_beam_600s\n",
      "2025-05-20 00:52:14,382 [INFO] Training hours file: whisper_training_hours.csv\n",
      "2025-05-20 00:52:14,382 [INFO] Output directory: analysis_results_beam\n",
      "2025-05-20 00:52:14,390 [INFO] Loaded training hours for 90 languages\n",
      "2025-05-20 00:52:14,391 [INFO] Found 28 potential language CSV files.\n",
      "2025-05-20 00:52:16,278 [INFO] Saved metrics to analysis_results_beam/language_confidence_metrics.csv\n",
      "2025-05-20 00:52:16,279 [INFO] Creating confidence plot...\n",
      "2025-05-20 00:52:16,299 [INFO] Correlation (log(training_hours) vs avg_confidence): r=0.314, p-value=1.778e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Correlation (log(training_hours) vs avg_confidence): r=0.314, p-value=1.778e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:52:17,114 [INFO] Saved confidence plot to analysis_results_beam (PNG, PDF, SVG)\n",
      "2025-05-20 00:52:17,115 [INFO] Confidence analysis complete!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Whisper Analysis: Average Confidence of Chosen Token vs. Training Hours\n",
    "=======================================================================\n",
    "\n",
    "Generates a plot of average model confidence (probability of the chosen token)\n",
    "against Whisper training hours for various languages.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Configuration\n",
    "# ────────────────────────────────────────────────────────────\n",
    "DATA_DIR = Path(\"results_beam_600s\")\n",
    "TRAINING_HOURS_CSV = Path(\"whisper_training_hours.csv\")\n",
    "OUTPUT_DIR = Path(\"analysis_results_beam\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Language resource groups\n",
    "HIGH_RESOURCE = {\"de\", \"es\", \"fr\", \"pt\", \"tr\"}\n",
    "MEDIUM_RESOURCE = {\"it\", \"nl\", \"sv\", \"ca\", \"fi\", \"id\", \"vi\", \"ro\",\n",
    "                   \"no\", \"cs\", \"hu\", \"yo\"}\n",
    "LOW_RESOURCE = {\"cy\", \"lt\", \"lv\", \"az\", \"et\", \"eu\", \"sq\", \"sw\", \"mt\", \"uz\"}\n",
    "\n",
    "EXCLUDED_LANGUAGES = {\"uz\", \"mt\", \"sw\", \"sq\", \"yo\", \"da\", \"vi\", \"cs\"}\n",
    "\n",
    "LANGUAGE_CODE_TO_NAME_MAP = {\n",
    "    \"de\": \"German\", \"es\": \"Spanish\", \"fr\": \"French\", \"pt\": \"Portuguese\", \"tr\": \"Turkish\",\n",
    "    \"it\": \"Italian\", \"nl\": \"Dutch\", \"sv\": \"Swedish\", \"ca\": \"Catalan\", \"fi\": \"Finnish\",\n",
    "    \"id\": \"Indonesian\", \"vi\": \"Vietnamese\", \"ro\": \"Romanian\", \"da\": \"Danish\",\n",
    "    \"no\": \"Norwegian\", \"cs\": \"Czech\", \"hu\": \"Hungarian\", \"yo\": \"Yoruba\",\n",
    "    \"cy\": \"Welsh\", \"lt\": \"Lithuanian\", \"lv\": \"Latvian\", \"az\": \"Azerbaijani\",\n",
    "    \"et\": \"Estonian\", \"eu\": \"Basque\", \"sq\": \"Albanian\", \"sw\": \"Swahili\",\n",
    "    \"mt\": \"Maltese\", \"uz\": \"Uzbek\",\n",
    "}\n",
    "\n",
    "# Visual styling\n",
    "COLOURS = {\"High\": \"steelblue\", \"Medium\": \"seagreen\",\n",
    "           \"Low\": \"crimson\", \"Other\": \"grey\"}\n",
    "MARKERS = {\"High\": \"o\", \"Medium\": \"^\", \"Low\": \"s\", \"Other\": \"x\"}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 100,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"font.size\": 8,\n",
    "    \"axes.titlesize\": 10,\n",
    "    \"axes.labelsize\": 8,\n",
    "    \"xtick.labelsize\": 7,\n",
    "    \"ytick.labelsize\": 7,\n",
    "    \"legend.fontsize\": 7,\n",
    "    \"figure.titlesize\": 12\n",
    "})\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Utility functions\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def get_resource_group(code):\n",
    "    \"\"\"Determine resource group based on language code.\"\"\"\n",
    "    if code in HIGH_RESOURCE:\n",
    "        return \"High\"\n",
    "    if code in MEDIUM_RESOURCE:\n",
    "        return \"Medium\"\n",
    "    if code in LOW_RESOURCE:\n",
    "        return \"Low\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "def load_training_hours():\n",
    "    \"\"\"Load training hours from CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(TRAINING_HOURS_CSV)\n",
    "        df = df[df[\"Whisper_Training_Hours\"].notna()]\n",
    "        df = df[df[\"Whisper_Training_Hours\"] != \"Unknown\"]\n",
    "        \n",
    "        hours_dict = {}\n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                code = str(row[\"Whisper_Code\"]).lower()\n",
    "                hours = float(row[\"Whisper_Training_Hours\"])\n",
    "                hours_dict[code] = hours\n",
    "            except (ValueError, KeyError):\n",
    "                logging.warning(f\"Skipping row due to missing code or invalid hours: {row}\")\n",
    "                continue\n",
    "            \n",
    "        logging.info(f\"Loaded training hours for {len(hours_dict)} languages\")\n",
    "        return hours_dict\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Training hours file not found: {TRAINING_HOURS_CSV}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load training hours: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def safe_read_csv(path):\n",
    "    \"\"\"Safely read a CSV file, handling potential errors.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(f\"File not found: {path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to read {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Core analysis functions\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def calculate_avg_confidence(df):\n",
    "    \"\"\"Calculate the average confidence (chosen_prob) of chosen tokens.\"\"\"\n",
    "    if 'step' in df.columns:\n",
    "        pred_rows = df[df['step'] >= 0].copy()\n",
    "    else:\n",
    "        logging.warning(\"No 'step' column found. Using all rows for confidence calculation.\")\n",
    "        pred_rows = df.copy()\n",
    "    \n",
    "    if pred_rows.empty:\n",
    "        logging.debug(\"No prediction rows found for confidence calculation\")\n",
    "        return np.nan\n",
    "    \n",
    "    if 'chosen_prob' not in pred_rows.columns:\n",
    "        logging.warning(\"Missing 'chosen_prob' column. Cannot calculate confidence.\")\n",
    "        return np.nan\n",
    "        \n",
    "    valid_probs = pred_rows['chosen_prob'].dropna()\n",
    "    if valid_probs.empty:\n",
    "        logging.debug(\"No valid 'chosen_prob' values found.\")\n",
    "        return np.nan\n",
    "        \n",
    "    return valid_probs.mean()\n",
    "\n",
    "\n",
    "def process_language_file(file_path, training_hours):\n",
    "    \"\"\"Process a single language file and extract metrics including confidence.\"\"\"\n",
    "    try:\n",
    "        filename = os.path.basename(file_path)\n",
    "        stem_parts = Path(filename).stem.split('_')\n",
    "        \n",
    "        lang_code_from_fn = None\n",
    "        if len(stem_parts) > 0 and len(stem_parts[0]) == 2 and not stem_parts[0].isdigit():\n",
    "            lang_code_from_fn = stem_parts[0].lower()\n",
    "            if len(stem_parts) > 1 and stem_parts[1] not in [\"subtoken\", \"beam\", \"wer\", \"results\"]:\n",
    "                if len(stem_parts) > 1:\n",
    "                    lang_code_from_fn = None\n",
    "\n",
    "        df = safe_read_csv(file_path)\n",
    "        \n",
    "        if df.empty:\n",
    "            logging.warning(f\"Empty or invalid data from {filename}\")\n",
    "            return None\n",
    "            \n",
    "        csv_lang_code = None\n",
    "        if 'whisper_lang' in df.columns:\n",
    "            unique_codes = df['whisper_lang'].dropna().astype(str).str.lower().unique()\n",
    "            if len(unique_codes) == 1:\n",
    "                csv_lang_code = unique_codes[0]\n",
    "            elif len(unique_codes) > 1:\n",
    "                logging.warning(f\"Multiple whisper_lang codes in {filename}: {unique_codes}. Using first: {unique_codes[0]}\")\n",
    "                csv_lang_code = unique_codes[0]\n",
    "        \n",
    "        final_lang_code = csv_lang_code if csv_lang_code else lang_code_from_fn\n",
    "        \n",
    "        display_language_name = \"Unknown\"\n",
    "        if final_lang_code and final_lang_code in LANGUAGE_CODE_TO_NAME_MAP:\n",
    "            display_language_name = LANGUAGE_CODE_TO_NAME_MAP[final_lang_code]\n",
    "        elif final_lang_code:\n",
    "            display_language_name = final_lang_code.capitalize()\n",
    "        else:\n",
    "            if len(stem_parts) > 0 and not stem_parts[0].isdigit() and len(stem_parts[0]) > 2:\n",
    "                display_language_name = stem_parts[0].capitalize()\n",
    "            elif len(stem_parts) > 1 and not stem_parts[1].isdigit():\n",
    "                display_language_name = stem_parts[1].capitalize()\n",
    "\n",
    "        logging.debug(f\"Processing {display_language_name} (Code: {final_lang_code if final_lang_code else 'N/A'}) from {filename}...\")\n",
    "\n",
    "        avg_confidence = calculate_avg_confidence(df)\n",
    "        \n",
    "        num_utterances = 0\n",
    "        if not df.empty:\n",
    "            if \"step\" in df.columns and (df[\"step\"] == -1).any():\n",
    "                num_utterances = int((df[\"step\"] == -1).sum())\n",
    "            elif not df.empty:\n",
    "                num_utterances = 1\n",
    "            else:\n",
    "                num_utterances = 0\n",
    "\n",
    "        return {\n",
    "            \"language\": display_language_name,\n",
    "            \"code\": final_lang_code,\n",
    "            \"training_hours\": training_hours.get(final_lang_code, np.nan) if final_lang_code else np.nan,\n",
    "            \"num_utterances\": num_utterances,\n",
    "            \"avg_confidence\": avg_confidence,\n",
    "            \"resource_group\": get_resource_group(final_lang_code) if final_lang_code else \"Other\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {file_path}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "def collect_all_results():\n",
    "    \"\"\"Collect results (including confidence) from all language files.\"\"\"\n",
    "    training_hours = load_training_hours()\n",
    "    \n",
    "    csv_files = sorted(DATA_DIR.glob(\"*_subtoken_beam.csv\"))\n",
    "\n",
    "    if not csv_files:\n",
    "        logging.error(f\"No relevant CSV files found in {DATA_DIR} (e.g., *_subtoken_beam.csv).\")\n",
    "        try:\n",
    "            all_files = list(DATA_DIR.glob(\"*.csv\"))\n",
    "            if all_files:\n",
    "                logging.info(f\"Found these CSV files: {', '.join(f.name for f in all_files[:10])}...\")\n",
    "            else:\n",
    "                logging.info(f\"Directory {DATA_DIR} contains no CSV files.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error listing directory contents: {e}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    logging.info(f\"Found {len(csv_files)} potential language CSV files.\")\n",
    "    \n",
    "    results = []\n",
    "    for file_path in csv_files:\n",
    "        result = process_language_file(file_path, training_hours)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    \n",
    "    if not results:\n",
    "        logging.error(\"No valid results collected from CSV files.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    try:\n",
    "        output_file = OUTPUT_DIR / \"language_confidence_metrics.csv\"\n",
    "        df.to_csv(output_file, index=False, float_format='%.4f')\n",
    "        logging.info(f\"Saved metrics to {output_file}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save metrics CSV: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Plotting functions\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def plot_confidence_vs_hours(results_df):\n",
    "    \"\"\"Plot average confidence vs training hours.\"\"\"\n",
    "    if 'code' not in results_df.columns:\n",
    "        logging.warning(\"'code' column not found in results_df. Skipping language exclusion for plotting.\")\n",
    "        df = results_df.copy()\n",
    "    else:\n",
    "        df = results_df[~results_df[\"code\"].isin(EXCLUDED_LANGUAGES)].copy()\n",
    "        \n",
    "    df = df.dropna(subset=[\"avg_confidence\", \"training_hours\"])\n",
    "    \n",
    "    if df.empty:\n",
    "        logging.warning(\"No valid data for average confidence plot after filtering and dropping NaNs.\")\n",
    "        return\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(4.5, 3.5))\n",
    "    \n",
    "    if 'resource_group' not in df.columns:\n",
    "        logging.warning(\"'resource_group' column not found. Plotting all points as 'Other'.\")\n",
    "        df['resource_group'] = 'Other'\n",
    "\n",
    "    for group_name in [\"High\", \"Medium\", \"Low\", \"Other\"]:\n",
    "        group_df = df[df[\"resource_group\"] == group_name]\n",
    "        if group_df.empty:\n",
    "            continue\n",
    "        \n",
    "        marker = MARKERS.get(group_name, \"x\")\n",
    "        color = COLOURS.get(group_name, \"grey\")\n",
    "\n",
    "        ax.scatter(\n",
    "            group_df[\"training_hours\"],\n",
    "            group_df[\"avg_confidence\"],\n",
    "            s=40,\n",
    "            alpha=0.7,\n",
    "            marker=marker,\n",
    "            color=color,\n",
    "            label=f\"{group_name}-resource\"\n",
    "        )\n",
    "        \n",
    "        if 'language' in group_df.columns:\n",
    "            for _, row in group_df.iterrows():\n",
    "                ax.annotate(\n",
    "                    row[\"language\"],\n",
    "                    (row[\"training_hours\"], row[\"avg_confidence\"]),\n",
    "                    xytext=(3, 1),\n",
    "                    textcoords=\"offset points\",\n",
    "                    fontsize=6,\n",
    "                )\n",
    "        else:\n",
    "            logging.warning(\"'language' column missing, skipping annotations.\")\n",
    "\n",
    "    # Calculate trend line and correlation\n",
    "    if len(df) >= 2:\n",
    "        valid_mask = (df[\"training_hours\"] > 0) & \\\n",
    "                     np.isfinite(df[\"training_hours\"].astype(float)) & \\\n",
    "                     np.isfinite(df[\"avg_confidence\"])\n",
    "        \n",
    "        if sum(valid_mask) >= 2:\n",
    "            x_log_values = np.log10(df.loc[valid_mask, \"training_hours\"].astype(float))\n",
    "            y_values = df.loc[valid_mask, \"avg_confidence\"]\n",
    "            \n",
    "            try:\n",
    "                r_val, p_value = pearsonr(x_log_values, y_values)\n",
    "                logging.info(f\"Correlation (log(training_hours) vs avg_confidence): r={r_val:.3f}, p-value={p_value:.3e}\")\n",
    "                print(f\"INFO: Correlation (log(training_hours) vs avg_confidence): r={r_val:.3f}, p-value={p_value:.3e}\")\n",
    "\n",
    "                z = np.polyfit(x_log_values, y_values, 1)\n",
    "                p_polyfit = np.poly1d(z)\n",
    "                \n",
    "                trend_label = f\"Trend (r={r_val:.2f}, p={p_value:.2e})\"\n",
    "\n",
    "                x_min = df.loc[valid_mask, \"training_hours\"].min()\n",
    "                x_max = df.loc[valid_mask, \"training_hours\"].max()\n",
    "\n",
    "                if x_min > 0 and x_max > 0 and x_min < x_max:\n",
    "                    x_line = np.logspace(np.log10(x_min), np.log10(x_max), 100)\n",
    "                    y_line = p_polyfit(np.log10(x_line))\n",
    "                    ax.plot(x_line, y_line, 'k--', linewidth=1, alpha=0.7, label=trend_label)\n",
    "                else:\n",
    "                    ax.plot([], [], 'k--', linewidth=1, alpha=0.7, label=trend_label)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error fitting trend line or calculating correlation: {e}\")\n",
    "                ax.plot([], [], 'k--', linewidth=1, alpha=0.7, label=\"Trend (error)\")\n",
    "        else:\n",
    "            logging.warning(\"Not enough valid data points to calculate trend line for confidence plot.\")\n",
    "            ax.plot([], [], 'k--', linewidth=1, alpha=0.7, label=\"Trend (insufficient data)\")\n",
    "    else:\n",
    "        ax.plot([], [], 'k--', linewidth=1, alpha=0.7, label=\"Trend (insufficient data)\")\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Whisper Training Hours (log scale)\")\n",
    "    ax.set_ylabel(\"Average Confidence of Chosen Token\")\n",
    "    \n",
    "    y_min_data = df[\"avg_confidence\"].min() if not df[\"avg_confidence\"].empty else np.nan\n",
    "    y_max_data = df[\"avg_confidence\"].max() if not df[\"avg_confidence\"].empty else np.nan\n",
    "\n",
    "    y_min_plot = max(0, y_min_data - 0.05) if pd.notna(y_min_data) else 0\n",
    "    y_max_plot = min(1, y_max_data + 0.05) if pd.notna(y_max_data) else 1\n",
    "    \n",
    "    if pd.notna(y_min_plot) and pd.notna(y_max_plot) and y_min_plot < y_max_plot:\n",
    "        ax.set_ylim(y_min_plot, y_max_plot)\n",
    "    else:\n",
    "        ax.set_ylim(0, 1)\n",
    "    \n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    ax.legend()\n",
    "    \n",
    "    fig.tight_layout(pad=0.5)\n",
    "    \n",
    "    try:\n",
    "        fig.savefig(OUTPUT_DIR / \"avg_confidence_vs_hours.png\")\n",
    "        fig.savefig(OUTPUT_DIR / \"avg_confidence_vs_hours.pdf\")\n",
    "        fig.savefig(OUTPUT_DIR / \"avg_confidence_vs_hours.svg\", format=\"svg\")\n",
    "        logging.info(f\"Saved confidence plot to {OUTPUT_DIR} (PNG, PDF, SVG)\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save confidence plot: {e}\")\n",
    "    \n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Main execution\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    logging.info(\"Starting average confidence analysis\")\n",
    "    logging.info(f\"Data directory: {DATA_DIR}\")\n",
    "    logging.info(f\"Training hours file: {TRAINING_HOURS_CSV}\")\n",
    "    logging.info(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    \n",
    "    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    results_df = collect_all_results()\n",
    "    \n",
    "    if results_df.empty:\n",
    "        logging.error(\"No usable results found from data collection. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    confidence_data_present = results_df[\"avg_confidence\"].notna().any()\n",
    "    if not confidence_data_present:\n",
    "        logging.warning(\"No valid average confidence data was found.\")\n",
    "        return\n",
    "        \n",
    "    logging.info(\"Creating confidence plot...\")\n",
    "    plot_confidence_vs_hours(results_df)\n",
    "    \n",
    "    logging.info(\"Confidence analysis complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (whisper_env)",
   "language": "python",
   "name": "whisper_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
