{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5798807-6939-4b28-9b1a-0203109a04f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Successfully loaded WhisperTokenizer for model: openai/whisper-base\n",
      "INFO: Starting GOLD TOKEN rank analysis\n",
      "INFO: Data directory: results_beam_600s\n",
      "INFO: Training hours file: whisper_training_hours.csv\n",
      "INFO: Output directory: analysis_results_beam\n",
      "INFO: Using Whisper Tokenizer for model: openai/whisper-base\n",
      "INFO: Max rank (K in Top-K) to check: 50\n",
      "INFO: Attempting to read ground truth from CSV column: 'ground_truth'\n",
      "INFO: Loaded training hours for 90 languages\n",
      "INFO: Found 28 files to process.\n",
      "INFO: Processing German (Code: de, File: 003_german_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 003_german_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Zieht euch bitte draußen die Schuhe aus....'\n",
      "INFO: Processing Spanish (Code: es, File: 004_spanish_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 004_spanish_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Habita en aguas poco profundas y rocosas....'\n",
      "INFO: Processing French (Code: fr, File: 007_french_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 007_french_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Ce dernier a évolué tout au long de l'histoire romaine....'\n",
      "INFO: Processing Portuguese (Code: pt, File: 009_portuguese_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 009_portuguese_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Quatro pessoas estão tocando instrumentos?, incluindo uma mulher vestida de preto tocando uma flauta...'\n",
      "INFO: Processing Turkish (Code: tr, File: 010_turkish_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 010_turkish_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Bir şeyler yapmalıyız....'\n",
      "INFO: Processing Italian (Code: it, File: 011_italian_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 011_italian_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Il libro ha suscitato molte polemiche a causa dei suoi contenuti....'\n",
      "INFO: Processing Dutch (Code: nl, File: 012_dutch_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 012_dutch_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Dat is de werkelijke uitdaging voor de komende jaren....'\n",
      "INFO: Processing Swedish (Code: sv, File: 013_swedish_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 013_swedish_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Barnet snyftade och skrek och skakade av köld....'\n",
      "INFO: Processing Catalan (Code: ca, File: 014_catalan_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 014_catalan_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Boca sense queixals, molí sense mola....'\n",
      "INFO: Processing Finnish (Code: fi, File: 015_finnish_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 015_finnish_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Kaikki jättäytyi näin minun varaani....'\n",
      "INFO: Processing Indonesian (Code: id, File: 016_indonesian_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 016_indonesian_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Maha Suci Allah....'\n",
      "INFO: Processing Vietnamese (Code: vi, File: 017_vietnamese_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 017_vietnamese_subtoken_beam.csv even after stripping whitespace. Using the first one: 'ạ. Dạ không ạ. Ngại quá...'\n",
      "INFO: Processing Romanian (Code: ro, File: 018_romanian_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 018_romanian_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Mica afacere a tatălui meu rămâne mică....'\n",
      "INFO: Processing Danish (Code: da, File: 019_danish_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 019_danish_subtoken_beam.csv even after stripping whitespace. Using the first one: 'seks...'\n",
      "INFO: Processing Norwegian (Code: no, File: 020_norwegian_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 020_norwegian_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Eg vert ikkje ferdig med han....'\n",
      "INFO: Processing Czech (Code: cs, File: 021_czech_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 021_czech_subtoken_beam.csv even after stripping whitespace. Using the first one: 'čtyři...'\n",
      "INFO: Processing Hungarian (Code: hu, File: 022_hungarian_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 022_hungarian_subtoken_beam.csv even after stripping whitespace. Using the first one: 'A téma természetesen nem mentes a vitáktól....'\n",
      "INFO: Processing Yoruba (Code: yo, File: 023_yoruba_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 023_yoruba_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Ó ti pé ọgọ́rùn-ún ọjọ́ tí ìjọba ti f'òfin de wá....'\n",
      "INFO: Processing Lithuanian (Code: lt, File: 035_lithuanian_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 035_lithuanian_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Aštuonios salos vadinamos „pagrindinėmis“ arba „didžiosiomis“ salomis....'\n",
      "INFO: Processing Welsh (Code: cy, File: 039_welsh_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 039_welsh_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Mae ganddi chwaer iau....'\n",
      "INFO: Processing Latvian (Code: lv, File: 043_latvian_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 043_latvian_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Līdzekļi budžetā ir, un runa ir tikai par šo līdzekļu pareizu sadali....'\n",
      "INFO: Processing Azerbaijani (Code: az, File: 046_azerbaijani_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 046_azerbaijani_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Eşq şeirlərindən əlavə din və təsəvvüf ilə bağlı şeirləri də vardır....'\n",
      "INFO: Processing Estonian (Code: et, File: 049_estonian_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 049_estonian_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Möödunud sajandil valitses Eestis mitukümmend aastat periood, kus keskkonnamürkide tõttu ei pesitsen...'\n",
      "INFO: Processing Basque (Code: eu, File: 052_basque_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 052_basque_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Hura ez zenuen ordea lehen pertsonan bizi....'\n",
      "INFO: Processing Albanian (Code: sq, File: 059_albanian_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 059_albanian_subtoken_beam.csv even after stripping whitespace. Using the first one: '\"Kontrollet po kryhen çdo ditë,\" thotë Rizahu....'\n",
      "INFO: Processing Swahili (Code: sw, File: 060_swahili_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 060_swahili_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Tuma pesa sahii....'\n",
      "INFO: Processing Uzbek (Code: uz, File: 079_uzbek_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 079_uzbek_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Xola, anuv shokoladingizdan obering, dedi chamasi etti yasharlar qiz bidirlab...'\n",
      "INFO: Processing Maltese (Code: mt, File: 085_maltese_subtoken_beam.csv)...\n",
      "WARNING: Multiple distinct ground truth strings found in column 'ground_truth' for 085_maltese_subtoken_beam.csv even after stripping whitespace. Using the first one: 'Tidher tassew ferħana li tkun omm ta' Owen....'\n",
      "INFO: Saved full metrics to analysis_results_beam/language_gold_token_rank_metrics_FULL.csv\n",
      "INFO: Successfully saved average rank data to analysis_results_beam/average_gold_token_rank_by_language.csv\n",
      "INFO: Creating plots based on GOLD token ranks...\n",
      "INFO: Correlation (log(training_hours) vs avg_rank): r=-0.355, p-value=1.246e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Correlation (log(training_hours) vs avg_rank): r=-0.355, p-value=1.246e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Saved gold rank vs hours plot to analysis_results_beam\n",
      "INFO: Saved gold ranks by language plot to analysis_results_beam\n",
      "INFO: Analysis complete! Results are in analysis_results_beam\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Whisper Analysis: Average Rank of GOLD Token in Model Candidates vs. Training Hours\n",
    "==================================================================================\n",
    "\n",
    "This script calculates the rank of ground truth (gold) subtokens within the\n",
    "model's prediction candidates. It requires ground truth transcriptions corresponding\n",
    "to the processed _subtoken_beam.csv files.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import logging\n",
    "import Levenshtein # For alignment\n",
    "from transformers import WhisperTokenizer # For Whisper's tokenizer\n",
    "from scipy.stats import pearsonr # <<<< NEW IMPORT FOR P-VALUE CALCULATION\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Hardcoded configuration\n",
    "# ────────────────────────────────────────────────────────────\n",
    "DATA_DIR = Path(\"results_beam_600s\")  # Directory with CSVs\n",
    "TRAINING_HOURS_CSV = Path(\"whisper_training_hours.csv\")  # CSV with training hours\n",
    "OUTPUT_DIR = Path(\"analysis_results_beam\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Define language resource groups\n",
    "HIGH_RESOURCE = {\"de\", \"es\", \"fr\", \"pt\", \"tr\"}\n",
    "MEDIUM_RESOURCE = {\"it\", \"nl\", \"sv\", \"ca\", \"fi\", \"id\", \"vi\", \"ro\", \"da\",\n",
    "                   \"no\", \"cs\", \"hu\"}\n",
    "LOW_RESOURCE = {\"cy\", \"lt\", \"lv\", \"az\", \"et\", \"eu\", \"sq\", \"sw\", \"mt\", \"uz\"}\n",
    "\n",
    "# Language code to full name mapping\n",
    "LANGUAGE_CODE_TO_NAME_MAP = {\n",
    "    \"de\": \"German\", \"es\": \"Spanish\", \"fr\": \"French\", \"pt\": \"Portuguese\", \"tr\": \"Turkish\",\n",
    "    \"it\": \"Italian\", \"nl\": \"Dutch\", \"sv\": \"Swedish\", \"ca\": \"Catalan\", \"fi\": \"Finnish\",\n",
    "    \"id\": \"Indonesian\", \"vi\": \"Vietnamese\", \"ro\": \"Romanian\", \"da\": \"Danish\",\n",
    "    \"no\": \"Norwegian\", \"cs\": \"Czech\", \"hu\": \"Hungarian\",\n",
    "    \"cy\": \"Welsh\", \"lt\": \"Lithuanian\", \"lv\": \"Latvian\", \"az\": \"Azerbaijani\",\n",
    "    \"et\": \"Estonian\", \"eu\": \"Basque\", \"sq\": \"Albanian\", \"sw\": \"Swahili\",\n",
    "    \"mt\": \"Maltese\", \"uz\": \"Uzbek\",\n",
    "    \"yo\": \"Yoruba\", # From EXCLUDED_LANGUAGES\n",
    "    # Add any other codes that might appear if necessary, ensuring they are lowercase\n",
    "}\n",
    "\n",
    "# Languages to exclude from plotting\n",
    "EXCLUDED_LANGUAGES = {\"uz\", \"mt\", \"sw\", \"sq\", \"yo\", \"da\", \"vi\", \"cs\"}\n",
    "\n",
    "# Maximum rank to check in candidates\n",
    "MAX_RANK = 50 # This is K in top-K\n",
    "\n",
    "# Whisper model name for tokenizer - ENSURE THIS MATCHES YOUR ASR MODEL\n",
    "MODEL_NAME = \"openai/whisper-base\" # E.g., \"openai/whisper-small\", \"openai/whisper-medium\"\n",
    "GROUND_TRUTH_COLUMN_NAME = \"ground_truth\" # <--- ASSUMED COLUMN NAME FOR GROUND TRUTH IN CSV\n",
    "\n",
    "# Visual styling for plots\n",
    "COLOURS = {\"High\": \"steelblue\", \"Medium\": \"seagreen\",\n",
    "           \"Low\": \"crimson\", \"Other\": \"grey\"}\n",
    "MARKERS = {\"High\": \"o\", \"Medium\": \"^\", \"Low\": \"s\", \"Other\": \"x\"}\n",
    "\n",
    "# --- MODIFICATION: Matplotlib rcParams for general appearance (font family removed) ---\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 100,\n",
    "    \"savefig.dpi\": 300,\n",
    "    # \"font.family\": \"Times New Roman\", # Removed font family enforcement\n",
    "    \"font.size\": 8,\n",
    "    \"axes.titlesize\": 10,\n",
    "    \"axes.labelsize\": 8,\n",
    "    \"xtick.labelsize\": 7,\n",
    "    \"ytick.labelsize\": 7,\n",
    "    \"legend.fontsize\": 7,\n",
    "    \"figure.titlesize\": 12\n",
    "})\n",
    "# --- END MODIFICATION ---\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Setup logging\n",
    "# ────────────────────────────────────────────────────────────\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Global Tokenizer Initialization\n",
    "# ────────────────────────────────────────────────────────────\n",
    "try:\n",
    "    GLOBAL_TOKENIZER = WhisperTokenizer.from_pretrained(MODEL_NAME)\n",
    "    logging.info(f\"Successfully loaded WhisperTokenizer for model: {MODEL_NAME}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load Whisper tokenizer ({MODEL_NAME}): {e}. \"\n",
    "                  \"Please ensure 'transformers' and 'sentencepiece' are installed, \"\n",
    "                  \"and the model name is correct.\")\n",
    "    exit()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Utility functions\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def get_resource_group(code):\n",
    "    \"\"\"Determine resource group based on language code.\"\"\"\n",
    "    if code in HIGH_RESOURCE: return \"High\"\n",
    "    if code in MEDIUM_RESOURCE: return \"Medium\"\n",
    "    if code in LOW_RESOURCE: return \"Low\"\n",
    "    return \"Other\"\n",
    "\n",
    "def load_training_hours():\n",
    "    \"\"\"Load training hours from CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(TRAINING_HOURS_CSV)\n",
    "        df = df[df[\"Whisper_Training_Hours\"].notna()]\n",
    "        df = df[df[\"Whisper_Training_Hours\"] != \"Unknown\"]\n",
    "        hours_dict = {str(row[\"Whisper_Code\"]).lower(): float(row[\"Whisper_Training_Hours\"]) # Ensure code is lowercase\n",
    "                      for _, row in df.iterrows()\n",
    "                      if pd.notna(row[\"Whisper_Code\"]) and pd.notna(row[\"Whisper_Training_Hours\"])}\n",
    "        logging.info(f\"Loaded training hours for {len(hours_dict)} languages\")\n",
    "        return hours_dict\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load training hours: {e}\")\n",
    "        return {}\n",
    "\n",
    "def safe_read_csv(path):\n",
    "    \"\"\"Safely read a CSV file, handling potential errors.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(f\"File not found: {path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to read {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Core analysis functions\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def calculate_rank_of_gold_subtokens(model_outputs_df, ground_truth_text, tokenizer, current_max_rank):\n",
    "    \"\"\"\n",
    "    Calculates the rank of gold standard subtokens in the model's predictions\n",
    "    for a single utterance.\n",
    "    \"\"\"\n",
    "    if model_outputs_df is None or model_outputs_df.empty:\n",
    "        logging.debug(\"Model output DataFrame is empty for rank calculation.\")\n",
    "        return np.nan, np.nan, np.nan, []\n",
    "    if not ground_truth_text:\n",
    "        logging.debug(\"Ground truth text is empty for rank calculation.\")\n",
    "        return np.nan, np.nan, np.nan, []\n",
    "    try:\n",
    "        gold_subtoken_ids = tokenizer(ground_truth_text, add_special_tokens=False).input_ids\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Tokenizer error on ground truth: '{str(ground_truth_text)[:50]}...': {e}\")\n",
    "        return np.nan, np.nan, np.nan, []\n",
    "    if not gold_subtoken_ids:\n",
    "        logging.debug(\"Ground truth tokenized to empty sequence.\")\n",
    "        return np.nan, np.nan, 0.0, []\n",
    "\n",
    "    model_chosen_ids = model_outputs_df['chosen_id'].dropna().astype(int).tolist()\n",
    "    top_k_cols = sorted(\n",
    "        [col for col in model_outputs_df.columns if col.startswith('top') and col.endswith('_id')],\n",
    "        key=lambda x: int(x.replace('top', '').replace('_id', ''))\n",
    "    )[:current_max_rank]\n",
    "\n",
    "    model_candidates_at_step = [[int(row[col]) for col in top_k_cols if pd.notna(row[col])]\n",
    "                                     for _, row in model_outputs_df.iterrows()]\n",
    "\n",
    "    if not model_chosen_ids:\n",
    "        ranks_for_gold_tokens = [current_max_rank + 1] * len(gold_subtoken_ids)\n",
    "        if not ranks_for_gold_tokens: return np.nan, np.nan, np.nan, []\n",
    "        avg_rank = np.mean(ranks_for_gold_tokens) if ranks_for_gold_tokens else np.nan\n",
    "        median_rank = np.median(ranks_for_gold_tokens) if ranks_for_gold_tokens else np.nan\n",
    "        percent_found = 0.0\n",
    "        return avg_rank, median_rank, percent_found, ranks_for_gold_tokens\n",
    "\n",
    "    opcodes = Levenshtein.opcodes(gold_subtoken_ids, model_chosen_ids)\n",
    "    ranks_for_gold_tokens = []\n",
    "    found_in_candidates_count = 0\n",
    "    total_gold_tokens_processed = 0\n",
    "\n",
    "    for tag, i1, i2, j1, j2 in opcodes:\n",
    "        if tag == 'equal' or tag == 'replace':\n",
    "            for k_op in range(i2 - i1):\n",
    "                gold_token_id = gold_subtoken_ids[i1 + k_op]\n",
    "                model_output_step_index = j1 + k_op\n",
    "                total_gold_tokens_processed += 1\n",
    "                current_rank_for_gold_token = current_max_rank + 1\n",
    "                if model_output_step_index < len(model_candidates_at_step):\n",
    "                    candidates_at_this_model_step = model_candidates_at_step[model_output_step_index]\n",
    "                    try:\n",
    "                        rank_0_indexed = candidates_at_this_model_step.index(gold_token_id)\n",
    "                        current_rank_for_gold_token = rank_0_indexed + 1\n",
    "                        if current_rank_for_gold_token <= current_max_rank:\n",
    "                            found_in_candidates_count += 1\n",
    "                    except ValueError: pass\n",
    "                ranks_for_gold_tokens.append(current_rank_for_gold_token)\n",
    "        elif tag == 'delete':\n",
    "            for _ in range(i2 - i1):\n",
    "                total_gold_tokens_processed += 1\n",
    "                ranks_for_gold_tokens.append(current_max_rank + 1)\n",
    "        elif tag == 'insert': pass\n",
    "\n",
    "    if not ranks_for_gold_tokens or total_gold_tokens_processed == 0:\n",
    "        return np.nan, np.nan, 0.0, []\n",
    "    avg_rank = np.mean(ranks_for_gold_tokens)\n",
    "    median_rank = np.median(ranks_for_gold_tokens)\n",
    "    percent_found = found_in_candidates_count / total_gold_tokens_processed\n",
    "    return avg_rank, median_rank, percent_found, ranks_for_gold_tokens\n",
    "\n",
    "def process_language_file(file_path, training_hours, tokenizer_instance, current_max_rank):\n",
    "    \"\"\"Process a single language file and extract metrics using gold token ranks.\"\"\"\n",
    "    try:\n",
    "        filename = os.path.basename(file_path)\n",
    "        stem_parts = Path(filename).stem.split('_')\n",
    "\n",
    "        lang_code_from_fn = None\n",
    "        if len(stem_parts) > 0 and len(stem_parts[0]) == 2:\n",
    "            lang_code_from_fn = stem_parts[0].lower()\n",
    "\n",
    "        df = safe_read_csv(file_path)\n",
    "        if df.empty:\n",
    "            logging.warning(f\"Empty or invalid data for file: {filename}\")\n",
    "            return None\n",
    "\n",
    "        csv_lang_code = None\n",
    "        if 'whisper_lang' in df.columns:\n",
    "            unique_codes = df['whisper_lang'].dropna().unique()\n",
    "            if len(unique_codes) == 1:\n",
    "                csv_lang_code = str(unique_codes[0]).lower()\n",
    "            elif len(unique_codes) > 1:\n",
    "                logging.warning(f\"Multiple whisper_lang codes in {filename}: {unique_codes}. Using first: {unique_codes[0]}\")\n",
    "                csv_lang_code = str(unique_codes[0]).lower()\n",
    "        \n",
    "        final_lang_code = csv_lang_code if csv_lang_code else lang_code_from_fn\n",
    "\n",
    "        display_language_name = \"Unknown\"\n",
    "        if final_lang_code:\n",
    "            display_language_name = LANGUAGE_CODE_TO_NAME_MAP.get(final_lang_code, final_lang_code.capitalize())\n",
    "        else:\n",
    "            if len(stem_parts) > 0:\n",
    "                name_candidate = stem_parts[0]\n",
    "                if name_candidate.lower() not in LANGUAGE_CODE_TO_NAME_MAP.keys() and \\\n",
    "                   name_candidate.capitalize() in LANGUAGE_CODE_TO_NAME_MAP.values():\n",
    "                    display_language_name = name_candidate.capitalize()\n",
    "                elif len(name_candidate) > 2 :\n",
    "                    display_language_name = name_candidate.capitalize()\n",
    "                elif name_candidate:\n",
    "                    display_language_name = LANGUAGE_CODE_TO_NAME_MAP.get(name_candidate.lower(), name_candidate.upper())\n",
    "            logging.warning(f\"Could not determine a definitive language code for {filename}. Attempting to use '{display_language_name}' based on filename parts.\")\n",
    "\n",
    "        logging.info(f\"Processing {display_language_name} (Code: {final_lang_code if final_lang_code else 'N/A'}, File: {filename})...\")\n",
    "\n",
    "        ground_truth_text = None\n",
    "        if GROUND_TRUTH_COLUMN_NAME in df.columns:\n",
    "            gts = df[GROUND_TRUTH_COLUMN_NAME].dropna()\n",
    "            if not gts.empty:\n",
    "                ground_truth_text = str(gts.iloc[0]).strip()\n",
    "                if gts.astype(str).str.strip().nunique() > 1:\n",
    "                    logging.warning(\n",
    "                        f\"Multiple distinct ground truth strings found in column '{GROUND_TRUTH_COLUMN_NAME}' \"\n",
    "                        f\"for {filename} even after stripping whitespace. Using the first one: '{ground_truth_text[:100]}...'\"\n",
    "                    )\n",
    "            else:\n",
    "                logging.warning(f\"Column '{GROUND_TRUTH_COLUMN_NAME}' found but contains no data for {filename}.\")\n",
    "        else:\n",
    "            logging.warning(f\"Ground truth column '{GROUND_TRUTH_COLUMN_NAME}' not found in {filename}. \")\n",
    "\n",
    "        if 'step' in df.columns: model_outputs_df = df[df['step'] >= 0].copy()\n",
    "        else:\n",
    "            logging.warning(f\"'step' column not found in {filename}. Assuming all rows are model outputs.\")\n",
    "            model_outputs_df = df.copy()\n",
    "\n",
    "        avg_rank, median_rank, percent_found = np.nan, np.nan, np.nan\n",
    "        if model_outputs_df.empty and 'step' in df.columns:\n",
    "            logging.warning(f\"No model output (prediction) rows found in {filename}.\")\n",
    "\n",
    "        if ground_truth_text is None:\n",
    "            logging.warning(f\"No ground truth text available for {filename}. Cannot calculate gold token ranks.\")\n",
    "        elif model_outputs_df.empty and not ground_truth_text:\n",
    "            avg_rank, median_rank, percent_found = 0.0, 0.0, 1.0\n",
    "            logging.info(f\"Both model output and ground truth are empty for {filename}.\")\n",
    "        elif model_outputs_df.empty and ground_truth_text:\n",
    "            logging.info(f\"Model output is empty but ground truth exists for {filename}.\")\n",
    "            try:\n",
    "                gold_ids = tokenizer_instance(ground_truth_text, add_special_tokens=False).input_ids\n",
    "                if gold_ids: avg_rank = median_rank = current_max_rank + 1.0; percent_found = 0.0\n",
    "                else: avg_rank, median_rank, percent_found = 0.0, 0.0, 1.0\n",
    "            except Exception as e_tok: logging.error(f\"Tokenizer error on GT for empty model output {filename}: {e_tok}\")\n",
    "        else:\n",
    "            avg_rank, median_rank, percent_found, _ = calculate_rank_of_gold_subtokens(\n",
    "                model_outputs_df, ground_truth_text, tokenizer_instance, current_max_rank\n",
    "            )\n",
    "\n",
    "        num_utterances = 0\n",
    "        if not df.empty:\n",
    "            if ground_truth_text is not None or not model_outputs_df.empty:\n",
    "                num_utterances = 1\n",
    "            if \"step\" in df.columns and (df[\"step\"] == -1).any():\n",
    "                num_utterances_step = int((df[\"step\"] == -1).sum())\n",
    "                if num_utterances_step > 0 : num_utterances = num_utterances_step\n",
    "\n",
    "        return {\"language\": display_language_name, \"code\": final_lang_code,\n",
    "                \"training_hours\": training_hours.get(final_lang_code, np.nan) if final_lang_code else np.nan,\n",
    "                \"num_utterances\": num_utterances, \"avg_rank\": avg_rank, \"median_rank\": median_rank,\n",
    "                \"percent_found\": percent_found, \n",
    "                \"resource_group\": get_resource_group(final_lang_code) if final_lang_code else \"Other\"}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {file_path}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def collect_all_results(tokenizer_instance, current_max_rank):\n",
    "    \"\"\"Collect results from all language files.\"\"\"\n",
    "    training_hours = load_training_hours()\n",
    "    csv_files = sorted(DATA_DIR.glob(\"*_subtoken_beam.csv\"))\n",
    "    if not csv_files:\n",
    "        logging.error(f\"No *_subtoken_beam.csv files found in {DATA_DIR}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    logging.info(f\"Found {len(csv_files)} files to process.\")\n",
    "    results = [res for file_path in csv_files if (res := process_language_file(file_path, training_hours, tokenizer_instance, current_max_rank)) is not None]\n",
    "    if not results:\n",
    "        logging.error(\"No valid results collected after processing all files.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    try:\n",
    "        output_file = OUTPUT_DIR / \"language_gold_token_rank_metrics_FULL.csv\" # MODIFIED: Changed name to avoid conflict\n",
    "        df_results.to_csv(output_file, index=False, float_format='%.4f')\n",
    "        logging.info(f\"Saved full metrics to {output_file}\")\n",
    "    except Exception as e: logging.error(f\"Failed to save full metrics: {e}\")\n",
    "    return df_results\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# NEW function to save average rank CSV\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def save_average_rank_csv(results_df):\n",
    "    \"\"\"Saves a CSV file with language, code, average rank, and resource group, sorted by average rank.\"\"\"\n",
    "    if results_df.empty or 'avg_rank' not in results_df.columns:\n",
    "        logging.warning(\"Results DataFrame is empty or 'avg_rank' column is missing. Cannot save average rank CSV.\")\n",
    "        return\n",
    "\n",
    "    df_avg_rank = results_df[['language', 'code', 'avg_rank', 'resource_group']].copy()\n",
    "    df_avg_rank = df_avg_rank.dropna(subset=['avg_rank'])\n",
    "    df_avg_rank = df_avg_rank.sort_values(by=\"avg_rank\", ascending=True)\n",
    "\n",
    "    if df_avg_rank.empty:\n",
    "        logging.warning(\"No data to save for average rank CSV after filtering NaNs.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        output_csv_path = OUTPUT_DIR / \"average_gold_token_rank_by_language.csv\"\n",
    "        df_avg_rank.to_csv(output_csv_path, index=False, float_format='%.4f')\n",
    "        logging.info(f\"Successfully saved average rank data to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save average rank CSV: {e}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Plotting functions\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def plot_avg_rank_vs_hours(results_df):\n",
    "    \"\"\"Plot average rank of GOLD token vs training hours.\"\"\"\n",
    "    df_copy = results_df.copy()\n",
    "    if 'code' not in df_copy.columns:\n",
    "        df_copy['code'] = None\n",
    "        \n",
    "    df = df_copy[~df_copy[\"code\"].isin(EXCLUDED_LANGUAGES)].copy()\n",
    "    df = df.dropna(subset=[\"avg_rank\", \"training_hours\", \"language\"])\n",
    "    \n",
    "    if df.empty:\n",
    "        logging.warning(\"No valid data for average gold rank plot after filtering.\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5.5, 4.0)) \n",
    "\n",
    "    for group_name, group_df in df.groupby(\"resource_group\"):\n",
    "        color = COLOURS.get(group_name, COLOURS[\"Other\"])\n",
    "        marker = MARKERS.get(group_name, MARKERS[\"Other\"])\n",
    "        # --- MODIFICATION: Language count removed from label ---\n",
    "        ax.scatter(group_df[\"training_hours\"], group_df[\"avg_rank\"], s=40, alpha=0.7,\n",
    "                   marker=marker, color=color, label=f\"{group_name}-resource\")\n",
    "        # --- END MODIFICATION ---\n",
    "        for _, row in group_df.iterrows():\n",
    "            ax.annotate(row[\"language\"], (row[\"training_hours\"], row[\"avg_rank\"]),\n",
    "                        xytext=(3, 1), textcoords=\"offset points\", fontsize=6, alpha=0.9)\n",
    "\n",
    "    # --- MODIFICATION FOR P-VALUE ---\n",
    "    correlation_text = \"N/A\"\n",
    "    if len(df) >= 2:\n",
    "        valid_mask = np.isfinite(df[\"training_hours\"]) & (df[\"training_hours\"] > 0) & np.isfinite(df[\"avg_rank\"])\n",
    "        if sum(valid_mask) >= 2:\n",
    "            x_log = np.log10(df.loc[valid_mask, \"training_hours\"])\n",
    "            y_vals = df.loc[valid_mask, \"avg_rank\"]\n",
    "            try:\n",
    "                # Calculate Pearson correlation and p-value\n",
    "                r, p_value = pearsonr(x_log, y_vals)\n",
    "                logging.info(f\"Correlation (log(training_hours) vs avg_rank): r={r:.3f}, p-value={p_value:.3e}\")\n",
    "                print(f\"INFO: Correlation (log(training_hours) vs avg_rank): r={r:.3f}, p-value={p_value:.3e}\") # <<< P-VALUE OUTPUT TO TERMINAL\n",
    "\n",
    "                z = np.polyfit(x_log, y_vals, 1); p_poly = np.poly1d(z) # Renamed 'p' to 'p_poly' to avoid conflict\n",
    "                \n",
    "                x_min, x_max = df.loc[valid_mask, \"training_hours\"].min(), df.loc[valid_mask, \"training_hours\"].max()\n",
    "                \n",
    "                trend_label = f\"Trend (r={r:.2f})\" # <<< UPDATED LABEL\n",
    "                \n",
    "                if x_min > 0 and x_max > 0 and x_min < x_max: \n",
    "                    x_line = np.logspace(np.log10(max(x_min * 0.9, 0.1)), np.log10(x_max * 1.1), 100)\n",
    "                    y_line = p_poly(np.log10(x_line))\n",
    "                    ax.plot(x_line, y_line, 'k--', alpha=0.7, linewidth=1, label=trend_label)\n",
    "                elif x_min == x_max and x_min > 0: # Handle case with only one distinct x value after log\n",
    "                    # Plotting a point or a short horizontal line might be tricky for trend\n",
    "                    # For now, we just show the correlation info. A trend line isn't meaningful here.\n",
    "                    logging.warning(f\"Only one distinct training hour value after log transformation for trend line: {x_min}. Trend line may not be representative.\")\n",
    "                    # Optionally, still add the label if you want to show r and p-value\n",
    "                    # ax.plot([], [], 'k--', alpha=0.7, linewidth=1, label=trend_label) # Invisible plot for label\n",
    "                else:\n",
    "                    logging.warning(\"Could not plot trend line due to insufficient range or invalid values in training hours.\")\n",
    "            except Exception as e_trend: \n",
    "                logging.warning(f\"Could not compute trend line or correlation: {e_trend}\")\n",
    "                print(f\"WARNING: Could not compute trend line or correlation: {e_trend}\") # <<< TERMINAL OUTPUT FOR ERROR\n",
    "    # --- END MODIFICATION FOR P-VALUE ---\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Whisper training hours (log scale)\")\n",
    "    ax.set_ylabel(f\"Average Rank of Correct Token\")\n",
    "    # ax.set_title(f\"Gold Token Rank in Candidates vs. Training Data Size (Top-{MAX_RANK})\")\n",
    "    y_max_val = df[\"avg_rank\"].max()\n",
    "    ax.set_ylim(0.9, min(MAX_RANK + 2, y_max_val * 1.1 if pd.notna(y_max_val) and y_max_val > 0 else MAX_RANK + 2))\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    ax.legend()\n",
    "    fig.tight_layout(pad=0.5)\n",
    "    try:\n",
    "        plt.savefig(OUTPUT_DIR / f\"avg_gold_rank_vs_hours_top{MAX_RANK}.png\")\n",
    "        plt.savefig(OUTPUT_DIR / f\"avg_gold_rank_vs_hours_top{MAX_RANK}.pdf\")\n",
    "        plt.savefig(OUTPUT_DIR / f\"avg_gold_rank_vs_hours_top{MAX_RANK}.svg\", format=\"svg\")\n",
    "        logging.info(f\"Saved gold rank vs hours plot to {OUTPUT_DIR}\")\n",
    "    except Exception as e: logging.error(f\"Failed to save gold rank vs hours plot: {e}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_ranks_by_language(results_df):\n",
    "    \"\"\"Plot ranked languages by average gold token rank.\"\"\"\n",
    "    df_copy = results_df.copy()\n",
    "    if 'code' not in df_copy.columns:\n",
    "        df_copy['code'] = None\n",
    "\n",
    "    df = df_copy[~df_copy[\"code\"].isin(EXCLUDED_LANGUAGES)].copy()\n",
    "    df = df.dropna(subset=[\"avg_rank\", \"median_rank\", \"language\"])\n",
    "    if df.empty:\n",
    "        logging.warning(\"No valid data for ranks by language plot after filtering.\")\n",
    "        return\n",
    "    df = df.sort_values(by=\"avg_rank\")\n",
    "    bar_colors = [COLOURS.get(grp, COLOURS[\"Other\"]) for grp in df[\"resource_group\"]]\n",
    "\n",
    "    fig_height = max(2.5, 0.18 * len(df)) \n",
    "    fig_width = 4.5 \n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "    y_pos = np.arange(len(df))\n",
    "    bars = ax.barh(y_pos, df[\"avg_rank\"], height=0.4, color=bar_colors, alpha=0.8, label=\"Avg Gold Rank\")\n",
    "    \n",
    "    for i, row_tuple in enumerate(df.itertuples()): \n",
    "        ax.text(row_tuple.avg_rank + 0.05 * df[\"avg_rank\"].max(), \n",
    "                i, f\"{row_tuple.avg_rank:.2f}\", va=\"center\", fontsize=6)\n",
    "        \n",
    "    y_tick_labels = []\n",
    "    for lang, code in zip(df[\"language\"], df[\"code\"]):\n",
    "        if pd.notna(code) and code != \"\":\n",
    "            y_tick_labels.append(f\"{lang} ({code})\")\n",
    "        else:\n",
    "            y_tick_labels.append(f\"{lang}\")\n",
    "            \n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(y_tick_labels) \n",
    "    ax.set_xlabel(f\"Rank of Gold Token in Top-{MAX_RANK} Candidates (lower is better)\")\n",
    "    ax.set_title(f\"Average Rank of Gold Token in Candidates by Language (Top-{MAX_RANK})\")\n",
    "    max_val = df[\"avg_rank\"].max()\n",
    "    ax.set_xlim(0.8, (max_val * 1.2) if pd.notna(max_val) and max_val > 0 else (MAX_RANK + 2)) \n",
    "    ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    from matplotlib.lines import Line2D\n",
    "    unique_groups_in_plot = df[\"resource_group\"].unique()\n",
    "    legend_elements = [Line2D([0], [0], marker='s', color='w', label=f'{grp}-resource',\n",
    "                              markerfacecolor=COLOURS.get(grp, COLOURS[\"Other\"]), markersize=8) \n",
    "                       for grp in COLOURS if grp in unique_groups_in_plot and grp != \"Other\"]\n",
    "    if \"Other\" in unique_groups_in_plot and \"Other\" in COLOURS :\n",
    "         legend_elements.append(Line2D([0], [0], marker='s', color='w', label='Other-resource',\n",
    "                                       markerfacecolor=COLOURS[\"Other\"], markersize=8))\n",
    "        \n",
    "    if legend_elements:\n",
    "        ax.legend(handles=legend_elements, loc=\"lower right\", title=\"Resource Groups\")\n",
    "    fig.tight_layout(pad=0.5)\n",
    "    try:\n",
    "        plt.savefig(OUTPUT_DIR / f\"gold_ranks_by_language_top{MAX_RANK}.png\") \n",
    "        plt.savefig(OUTPUT_DIR / f\"gold_ranks_by_language_top{MAX_RANK}.pdf\")\n",
    "        logging.info(f\"Saved gold ranks by language plot to {OUTPUT_DIR}\")\n",
    "    except Exception as e: logging.error(f\"Failed to save gold ranks by language plot: {e}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# Main execution\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    logging.info(\"Starting GOLD TOKEN rank analysis\")\n",
    "    logging.info(f\"Data directory: {DATA_DIR}\")\n",
    "    logging.info(f\"Training hours file: {TRAINING_HOURS_CSV}\")\n",
    "    logging.info(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    logging.info(f\"Using Whisper Tokenizer for model: {MODEL_NAME}\")\n",
    "    logging.info(f\"Max rank (K in Top-K) to check: {MAX_RANK}\")\n",
    "    logging.info(f\"Attempting to read ground truth from CSV column: '{GROUND_TRUTH_COLUMN_NAME}'\")\n",
    "\n",
    "    if not hasattr(GLOBAL_TOKENIZER, 'encode'): \n",
    "        logging.error(\"GLOBAL_TOKENIZER is not properly initialized. Exiting.\")\n",
    "        return\n",
    "\n",
    "    results_df = collect_all_results(GLOBAL_TOKENIZER, MAX_RANK)\n",
    "    if results_df.empty:\n",
    "        logging.error(\"No usable results found after processing all files. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Save the average rank CSV\n",
    "    save_average_rank_csv(results_df) # <-- ADDED CALL HERE\n",
    "\n",
    "    if not results_df[\"avg_rank\"].notna().any():\n",
    "        logging.warning(\"No valid rank data was calculated. Plots will not be generated. \"\n",
    "                        \"Check logs for issues with ground truth or data processing.\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"Creating plots based on GOLD token ranks...\")\n",
    "    plot_avg_rank_vs_hours(results_df)\n",
    "    plot_ranks_by_language(results_df)\n",
    "    logging.info(f\"Analysis complete! Results are in {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (whisper_env)",
   "language": "python",
   "name": "whisper_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
